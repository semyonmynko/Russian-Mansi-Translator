{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9354843,"sourceType":"datasetVersion","datasetId":5671068}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sacrebleu\n!pip install peft","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-10T11:18:03.564722Z","iopub.execute_input":"2024-09-10T11:18:03.565008Z","iopub.status.idle":"2024-09-10T11:18:31.938720Z","shell.execute_reply.started":"2024-09-10T11:18:03.564970Z","shell.execute_reply":"2024-09-10T11:18:31.937599Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.10.1 sacrebleu-2.4.3\nCollecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.12.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom torch.utils.data import Dataset, DataLoader\nimport datasets\nfrom transformers import AdamW\nimport torch.nn as nn\nfrom torch.cuda.amp import GradScaler, autocast\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:18:31.940707Z","iopub.execute_input":"2024-09-10T11:18:31.941026Z","iopub.status.idle":"2024-09-10T11:18:38.041753Z","shell.execute_reply.started":"2024-09-10T11:18:31.940993Z","shell.execute_reply":"2024-09-10T11:18:38.040952Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM\n\nconfig = PeftConfig.from_pretrained(\"KapitalK/mGPT\")\nbase_model = AutoModelForCausalLM.from_pretrained(\"ai-forever/mGPT\")\nmodel = PeftModel.from_pretrained(base_model, \"KapitalK/mGPT\").to('cuda')\ntokenizer = AutoTokenizer.from_pretrained(\"ai-forever/mGPT\")\n\n# model_name = \"ai-forever/mGPT\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:18:38.042875Z","iopub.execute_input":"2024-09-10T11:18:38.043408Z","iopub.status.idle":"2024-09-10T11:19:46.497512Z","shell.execute_reply.started":"2024-09-10T11:18:38.043372Z","shell.execute_reply":"2024-09-10T11:19:46.496666Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/495 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff03361791cf4ee6a6149260f7267e1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/738 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41fb687c500c48d7805ba08179994123"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40b7eaa4e87b4eb8ab75ee94f5bbbff4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/66.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1a6c814fa9145d7a249d91ed9e0d3dc"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  adapters_weights = torch.load(filename, map_location=torch.device(device))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c3d6af500d34f59a6dcf1d185de6012"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.89M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6491f1548c2c4f2381f0ce0b07a71f56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.20M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01859a73ce8f46efbd2a00b7c8097a57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/606 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91c552cf62a049028695cd35e4749cdc"}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = tokenizer(examples['source'], max_length=128, truncation=True, padding='max_length')\n    targets = tokenizer(examples['target'], max_length=128, truncation=True, padding='max_length')\n    inputs['labels'] = targets['input_ids']\n    return inputs\n\ndataset = pd.read_csv(\"/kaggle/input/mansi-russian-parralel-corpus/overall_80K.csv\", index_col=0)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:19:46.498650Z","iopub.execute_input":"2024-09-10T11:19:46.499155Z","iopub.status.idle":"2024-09-10T11:19:47.077717Z","shell.execute_reply.started":"2024-09-10T11:19:46.499110Z","shell.execute_reply":"2024-09-10T11:19:47.076865Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"texts = dataset['source'].tolist() + dataset['target'].tolist()\nunique_tokens = set()\nfor text in tqdm(texts):\n    tokens = tokenizer.tokenize(text)\n    unique_tokens.update(tokens)\n    \nnum_added_tokens = tokenizer.add_tokens(list(unique_tokens))\n\nprint(f\"Added {num_added_tokens} new tokens.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:19:47.080046Z","iopub.execute_input":"2024-09-10T11:19:47.080397Z","iopub.status.idle":"2024-09-10T11:20:09.828488Z","shell.execute_reply.started":"2024-09-10T11:19:47.080363Z","shell.execute_reply":"2024-09-10T11:20:09.827565Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"100%|██████████| 162292/162292 [00:22<00:00, 7233.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Added 10584 new tokens.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:20:09.829777Z","iopub.execute_input":"2024-09-10T11:20:09.830190Z","iopub.status.idle":"2024-09-10T11:20:09.856912Z","shell.execute_reply.started":"2024-09-10T11:20:09.830128Z","shell.execute_reply":"2024-09-10T11:20:09.855923Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Embedding(100000, 2048)"},"metadata":{}}]},{"cell_type":"code","source":"train_df, test_val_df = train_test_split(dataset, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(test_val_df, test_size=0.5, random_state=42)\n\ntrain_dataset = datasets.Dataset.from_pandas(train_df)\nval_dataset = datasets.Dataset.from_pandas(val_df)\ntest_dataset = datasets.Dataset.from_pandas(test_df)\n\ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\nval_dataset = val_dataset.map(preprocess_function, batched=True)\ntest_dataset = test_dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:20:09.858225Z","iopub.execute_input":"2024-09-10T11:20:09.858608Z","iopub.status.idle":"2024-09-10T11:20:27.198479Z","shell.execute_reply.started":"2024-09-10T11:20:09.858565Z","shell.execute_reply":"2024-09-10T11:20:27.197473Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/64916 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aac86a5bbf240899947b9fb0e53da67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8115 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb5950790f1b4bbcb8fb2ac82ebdee22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8115 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8acd65e10054134b22b05a956f45ab2"}},"metadata":{}}]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        return {\n            'input_ids': torch.tensor(item['input_ids']),\n            'attention_mask': torch.tensor(item['attention_mask']),\n            'labels': torch.tensor(item['labels'])\n        }\n\ntrain_loader = DataLoader(CustomDataset(train_dataset), batch_size=8, shuffle=True)\neval_loader = DataLoader(CustomDataset(test_dataset), batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:20:27.199645Z","iopub.execute_input":"2024-09-10T11:20:27.199942Z","iopub.status.idle":"2024-09-10T11:20:27.207057Z","shell.execute_reply.started":"2024-09-10T11:20:27.199909Z","shell.execute_reply":"2024-09-10T11:20:27.206092Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndef train(model, train_loader, optimizer, criterion, scaler):\n    model.train()\n    epoch_loss = 0\n\n    for i, batch in tqdm(enumerate(train_loader), total=len(train_loader), position=0, leave=True):\n        input_ids = batch['input_ids'].to('cuda')\n        attention_mask = batch['attention_mask'].to('cuda')\n        labels = batch['labels'].to('cuda')\n\n        optimizer.zero_grad()\n\n        with autocast():\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        epoch_loss += loss.item()\n        \n        del input_ids, attention_mask, labels\n        torch.cuda.empty_cache()\n        \n        if i%100==0:\n            print(f'Step {i + 1}, Train Loss: {epoch_loss / (i+1):.4f}')\n\n    return epoch_loss / len(train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:20:27.208443Z","iopub.execute_input":"2024-09-10T11:20:27.209267Z","iopub.status.idle":"2024-09-10T11:20:27.221124Z","shell.execute_reply.started":"2024-09-10T11:20:27.209217Z","shell.execute_reply":"2024-09-10T11:20:27.220286Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=5e-5)\nscaler = GradScaler()","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:20:27.222274Z","iopub.execute_input":"2024-09-10T11:20:27.222637Z","iopub.status.idle":"2024-09-10T11:20:27.691367Z","shell.execute_reply.started":"2024-09-10T11:20:27.222605Z","shell.execute_reply":"2024-09-10T11:20:27.690577Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/tmp/ipykernel_36/2643206744.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"}]},{"cell_type":"code","source":"n_epochs = 1\n\nfor epoch in tqdm(range(n_epochs)):\n    train_loss = train(model, train_loader, optimizer, nn.CrossEntropyLoss(), scaler)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:20:27.692436Z","iopub.execute_input":"2024-09-10T11:20:27.692907Z","iopub.status.idle":"2024-09-10T11:32:54.477966Z","shell.execute_reply.started":"2024-09-10T11:20:27.692874Z","shell.execute_reply":"2024-09-10T11:32:54.476632Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"  0%|          | 0/8115 [00:00<?, ?it/s]/tmp/ipykernel_36/1000605197.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n  0%|          | 1/8115 [00:01<4:14:53,  1.88s/it]","output_type":"stream"},{"name":"stdout","text":"Step 1, Train Loss: 26.2023\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 101/8115 [02:21<3:06:06,  1.39s/it]","output_type":"stream"},{"name":"stdout","text":"Step 101, Train Loss: 23.5420\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 201/8115 [04:40<3:03:49,  1.39s/it]","output_type":"stream"},{"name":"stdout","text":"Step 201, Train Loss: 22.2644\n","output_type":"stream"},{"name":"stderr","text":"  4%|▎         | 301/8115 [07:00<3:01:47,  1.40s/it]","output_type":"stream"},{"name":"stdout","text":"Step 301, Train Loss: 21.3547\n","output_type":"stream"},{"name":"stderr","text":"  5%|▍         | 401/8115 [09:19<2:59:11,  1.39s/it]","output_type":"stream"},{"name":"stdout","text":"Step 401, Train Loss: 20.6402\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 501/8115 [11:38<2:56:54,  1.39s/it]","output_type":"stream"},{"name":"stdout","text":"Step 501, Train Loss: 20.0456\n","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 534/8115 [12:26<2:56:32,  1.40s/it]\n  0%|          | 0/1 [12:26<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_epochs)):\n\u001b[0;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[9], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, scaler)\u001b[0m\n\u001b[1;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     19\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 20\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     23\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:454\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    452\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 454\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import random\nfrom tqdm import tqdm\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\nrandom.seed(42) \n# random_indices = random.sample(range(len(test_dataset)), 1000)\nrandom_test_samples = test_dataset\n\ndef generate_translation(sample):\n    input_ids = torch.tensor(sample['input_ids']).unsqueeze(0).to(device)  # Конвертируем список в тензор\n    with torch.no_grad():\n        generated_ids = model.generate(input_ids, max_length=200, num_beams=5, early_stopping=True)\n    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\noriginal_texts = []\ncorrect_translations = []\nmodel_translations = []\n\nfor i, sample in tqdm(enumerate(random_test_samples)):\n    input_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n    correct_translation = tokenizer.decode(sample['labels'], skip_special_tokens=True)\n    model_translation = generate_translation(sample)\n\n    original_texts.append(input_text)\n    correct_translations.append(correct_translation)\n    model_translations.append(model_translation)\n    break\n    \ndf_results = pd.DataFrame({\n    \"Original Text\": original_texts,\n    \"Correct Translation\": correct_translations,\n    \"Model Translation\": model_translations\n})","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:32:57.006912Z","iopub.execute_input":"2024-09-10T11:32:57.007327Z","iopub.status.idle":"2024-09-10T11:33:13.427323Z","shell.execute_reply.started":"2024-09-10T11:32:57.007288Z","shell.execute_reply":"2024-09-10T11:33:13.426203Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1685: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n0it [00:16, ?it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"df_results","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:33:13.429303Z","iopub.execute_input":"2024-09-10T11:33:13.430178Z","iopub.status.idle":"2024-09-10T11:33:13.442895Z","shell.execute_reply.started":"2024-09-10T11:33:13.430108Z","shell.execute_reply":"2024-09-10T11:33:13.441941Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                      Original Text                 Correct Translation  \\\n0  Пётр всего полведра воды принёс.  Петыр вēтра суп вит туп та тотас.    \n\n                                   Model Translation  \n0  ლებშილებშილებშილებშილებშილებშილებშილებშილებშილ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Original Text</th>\n      <th>Correct Translation</th>\n      <th>Model Translation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Пётр всего полведра воды принёс.</td>\n      <td>Петыр вēтра суп вит туп та тотас.</td>\n      <td>ლებშილებშილებშილებშილებშილებშილებშილებშილებშილ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
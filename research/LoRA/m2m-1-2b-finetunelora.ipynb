{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9326296,"sourceType":"datasetVersion","datasetId":5650154}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install peft\n!pip install sacrebleu","metadata":{"execution":{"iopub.status.busy":"2024-09-08T20:06:55.075577Z","iopub.execute_input":"2024-09-08T20:06:55.076408Z","iopub.status.idle":"2024-09-08T20:07:24.174354Z","shell.execute_reply.started":"2024-09-08T20:06:55.076361Z","shell.execute_reply":"2024-09-08T20:07:24.173300Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.12.0\nCollecting sacrebleu\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.10.1 sacrebleu-2.4.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\nfrom peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer","metadata":{"execution":{"iopub.status.busy":"2024-09-08T20:07:24.176668Z","iopub.execute_input":"2024-09-08T20:07:24.177034Z","iopub.status.idle":"2024-09-08T20:07:44.977472Z","shell.execute_reply.started":"2024-09-08T20:07:24.176999Z","shell.execute_reply":"2024-09-08T20:07:44.976672Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model_name = \"facebook/m2m100_1.2B\"\ntokenizer = M2M100Tokenizer.from_pretrained(model_name)\nmodel = M2M100ForConditionalGeneration.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T20:07:44.978518Z","iopub.execute_input":"2024-09-08T20:07:44.979148Z","iopub.status.idle":"2024-09-08T20:08:12.457700Z","shell.execute_reply.started":"2024-09-08T20:07:44.979111Z","shell.execute_reply":"2024-09-08T20:08:12.456808Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09f1a818b881463b96610d7fb4051edf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1ffb15ac30042f09bc429a67cf10c97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11a60d801fbe460885fb32ca05833453"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959d2e9b504e415aab55183441228375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/909 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6d6d2d3b3c840c8af597a8f5ee6841a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee801c839c0b422fb6c816fc9b259fce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ec172efd7d6487f8347faf83a57780c"}},"metadata":{}}]},{"cell_type":"code","source":"special_tokens = {\"additional_special_tokens\": [\"__mns__\"]}\ntokenizer.add_special_tokens(special_tokens)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-09-08T20:08:12.459824Z","iopub.execute_input":"2024-09-08T20:08:12.460150Z","iopub.status.idle":"2024-09-08T20:08:15.186834Z","shell.execute_reply.started":"2024-09-08T20:08:12.460117Z","shell.execute_reply":"2024-09-08T20:08:15.185975Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"M2M100ScaledWordEmbedding(128105, 1024, padding_idx=1)"},"metadata":{}}]},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,  \n    lora_alpha=32,  \n    target_modules=[\"q_proj\", \"v_proj\"],  \n    lora_dropout=0.1,  \n    bias=\"none\", \n    task_type=\"SEQ_2_SEQ_LM\"  \n)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T20:08:15.187998Z","iopub.execute_input":"2024-09-08T20:08:15.188303Z","iopub.status.idle":"2024-09-08T20:08:15.193058Z","shell.execute_reply.started":"2024-09-08T20:08:15.188272Z","shell.execute_reply":"2024-09-08T20:08:15.192030Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T20:08:15.194273Z","iopub.execute_input":"2024-09-08T20:08:15.194556Z","iopub.status.idle":"2024-09-08T20:08:15.449177Z","shell.execute_reply.started":"2024-09-08T20:08:15.194525Z","shell.execute_reply":"2024-09-08T20:08:15.448235Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport sacrebleu","metadata":{"execution":{"iopub.status.busy":"2024-09-08T20:08:15.450642Z","iopub.execute_input":"2024-09-08T20:08:15.451432Z","iopub.status.idle":"2024-09-08T20:08:15.546622Z","shell.execute_reply.started":"2024-09-08T20:08:15.451386Z","shell.execute_reply":"2024-09-08T20:08:15.545740Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/mansi-russian-parralel-corpus/overall_80K.csv\", index_col=0)\ntrain_df, test_val_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(test_val_df, test_size=0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T20:08:15.548094Z","iopub.execute_input":"2024-09-08T20:08:15.548651Z","iopub.status.idle":"2024-09-08T20:08:16.152666Z","shell.execute_reply.started":"2024-09-08T20:08:15.548618Z","shell.execute_reply":"2024-09-08T20:08:16.151473Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T20:08:16.154156Z","iopub.execute_input":"2024-09-08T20:08:16.154674Z","iopub.status.idle":"2024-09-08T20:08:16.354254Z","shell.execute_reply.started":"2024-09-08T20:08:16.154635Z","shell.execute_reply":"2024-09-08T20:08:16.353278Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples):\n    targets = [f\"__mns__ {inp}\" for inp in examples['target']]\n    inputs = [f\"__ru__ {inp}\" for inp in examples['source']]\n    \n    inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=128)\n    targets = tokenizer(targets, truncation=True, padding='max_length', max_length=128)\n    inputs['labels'] = targets['input_ids']\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2024-09-08T20:08:16.358091Z","iopub.execute_input":"2024-09-08T20:08:16.358482Z","iopub.status.idle":"2024-09-08T20:08:16.505424Z","shell.execute_reply.started":"2024-09-08T20:08:16.358447Z","shell.execute_reply":"2024-09-08T20:08:16.503979Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.map(preprocess_function, batched=True)\nval_dataset = val_dataset.map(preprocess_function, batched=True)\ntest_dataset = test_dataset.map(preprocess_function, batched=True)\n\ntrain_dataset = train_dataset.remove_columns(['source', 'target', '__index_level_0__'])\nval_dataset = val_dataset.remove_columns(['source', 'target', '__index_level_0__'])\ntest_dataset = test_dataset.remove_columns(['source', 'target', '__index_level_0__'])","metadata":{"execution":{"iopub.status.busy":"2024-09-08T20:08:16.506829Z","iopub.execute_input":"2024-09-08T20:08:16.507241Z","iopub.status.idle":"2024-09-08T20:09:00.047266Z","shell.execute_reply.started":"2024-09-08T20:08:16.507197Z","shell.execute_reply":"2024-09-08T20:09:00.046264Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/64916 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51055743931f4a44acd63cf1d579f038"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8115 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c16373f69542486fafbd54ba951cd0b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8115 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e216344dc484851998c12d5e55539bf"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T20:09:00.048727Z","iopub.execute_input":"2024-09-08T20:09:00.049573Z","iopub.status.idle":"2024-09-08T20:09:00.053631Z","shell.execute_reply.started":"2024-09-08T20:09:00.049524Z","shell.execute_reply":"2024-09-08T20:09:00.052637Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./m2m-1B-finetune\",\n    evaluation_strategy=\"no\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4, \n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_strategy=\"no\", \n    predict_with_generate=True,\n    fp16=True,  \n    logging_dir=\"./logs\",\n    report_to=\"none\", \n    load_best_model_at_end=False, \n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T20:09:00.054944Z","iopub.execute_input":"2024-09-08T20:09:00.055385Z","iopub.status.idle":"2024-09-09T03:06:41.833639Z","shell.execute_reply.started":"2024-09-08T20:09:00.055337Z","shell.execute_reply":"2024-09-09T03:06:41.832831Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12171' max='12171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12171/12171 6:57:35, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>7.674300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>6.553800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>6.454200</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>6.398000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>6.377500</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>6.341000</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>6.320600</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>6.301800</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>6.283600</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>6.268500</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>6.264200</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>6.259900</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>6.232800</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>6.253300</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>6.231700</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>6.220300</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>6.212700</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>6.219600</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>6.193200</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>6.197500</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>6.194900</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>6.187800</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>6.188300</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>6.187900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=12171, training_loss=6.331619394333868, metrics={'train_runtime': 25058.3739, 'train_samples_per_second': 7.772, 'train_steps_per_second': 0.486, 'total_flos': 1.6645751374558003e+17, 'train_loss': 6.331619394333868, 'epoch': 2.999815145726785})"},"metadata":{}}]},{"cell_type":"code","source":"model.eval();\n!mkdir m2m_1B_finetune\n!mkdir m2m_1B_finetune/model\n!mkdir m2m_1B_finetune/tokenizer\nmodel.save_pretrained(\"m2m_1B_finetune/model/\")\ntokenizer.save_pretrained(\"m2m_1B_finetune/tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-09-09T03:06:41.834806Z","iopub.execute_input":"2024-09-09T03:06:41.835138Z","iopub.status.idle":"2024-09-09T03:06:47.921623Z","shell.execute_reply.started":"2024-09-09T03:06:41.835105Z","shell.execute_reply":"2024-09-09T03:06:47.920627Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('m2m_1B_finetune/tokenizer/tokenizer_config.json',\n 'm2m_1B_finetune/tokenizer/special_tokens_map.json',\n 'm2m_1B_finetune/tokenizer/vocab.json',\n 'm2m_1B_finetune/tokenizer/sentencepiece.bpe.model',\n 'm2m_1B_finetune/tokenizer/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\nshutil.make_archive(\"m2m_1B_finetune\", 'zip', \"m2m_1B_finetune/\")\nFileLink('m2m_1B_finetune.zip')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T03:06:47.923208Z","iopub.execute_input":"2024-09-09T03:06:47.924331Z","iopub.status.idle":"2024-09-09T03:11:12.212646Z","shell.execute_reply.started":"2024-09-09T03:06:47.924284Z","shell.execute_reply":"2024-09-09T03:11:12.211807Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/m2m_1B_finetune.zip","text/html":"<a href='m2m_1B_finetune.zip' target='_blank'>m2m_1B_finetune.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"import random\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nrandom.seed(42) \nrandom_test_samples = test_dataset\nmodel.eval()\n\ndef generate_translation(sample):\n    input_ids = torch.tensor(sample['input_ids']).to(device)  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –≤ —Ç–µ–Ω–∑–æ—Ä\n    with torch.no_grad():\n        generated_ids = model.generate(input_ids)\n    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\noriginal_texts = []\ncorrect_translations = []\nmodel_translations = []\n\nfor sample in tqdm(random_test_samples):\n    input_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n    correct_translation = tokenizer.decode(sample['labels'], skip_special_tokens=True)\n    model_translation = generate_translation(sample)\n\n    original_texts.append(input_text)\n    correct_translations.append(correct_translation)\n    model_translations.append(model_translation)\n    \ndf_results = pd.DataFrame({\n    \"Original Text\": original_texts,\n    \"Correct Translation\": correct_translations,\n    \"Model Translation\": model_translations\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results.head(50)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T03:11:12.589007Z","iopub.status.idle":"2024-09-09T03:11:12.589359Z","shell.execute_reply.started":"2024-09-09T03:11:12.589189Z","shell.execute_reply":"2024-09-09T03:11:12.589207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results.loc[:, 'Original Text'] = df_results['Original Text'].str.replace('__en__', '')\ndf_results.loc[:, 'Correct Translation'] = df_results['Correct Translation'].str.replace('__en__', '')\ndf_results.loc[:, 'Model Translation'] = df_results['Model Translation'].str.replace('__en__', '')\n    \nbleu_score = sacrebleu.corpus_bleu(df_results['Model Translation'].tolist(), \n                                   [df_results['Correct Translation'].tolist()]).score\nchrf_score = sacrebleu.corpus_chrf(df_results['Model Translation'].tolist(), \n                                   [df_results['Correct Translation'].tolist()]).score\n\nprint(f\"BLEU Score: {bleu_score}\")\nprint(f\"ChrF Score: {chrf_score}\")\n\ndf_results.head(50)","metadata":{},"execution_count":null,"outputs":[]}]}